# Ceph

Ceph æ˜¯ä¸€ä¸ªå¼€æºçš„åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼Œæ”¯æŒå¯¹è±¡å­˜å‚¨ã€å—å­˜å‚¨å’Œæ–‡ä»¶ç³»ç»Ÿä¸‰ç§æ¥å£ï¼Œå…·æœ‰é«˜å¯ç”¨æ€§ã€é«˜æ‰©å±•æ€§å’Œå¼ºä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡ CRUSH ç®—æ³•å®ç°å»ä¸­å¿ƒåŒ–æ•°æ®åˆ†å¸ƒï¼Œæ— éœ€ä¸“ç”¨ç¡¬ä»¶å³å¯æ„å»ºå¤§è§„æ¨¡å¯é å­˜å‚¨é›†ç¾¤ï¼Œå¹¿æ³›åº”ç”¨äºäº‘è®¡ç®—å’Œå¤§æ•°æ®åœºæ™¯ã€‚

- [å®˜ç½‘æ–‡æ¡£](https://docs.ceph.com/en/latest/releases/)
- [å®‰è£…æ–‡æ¡£](https://docs.ceph.com/en/latest/cephadm/install/#cephadm-deploying-new-cluster)

- [Github](https://github.com/ceph/ceph/tree/quincy/src/cephadm)

## æœåŠ¡æ¦‚è§ˆ

### Ceph æœåŠ¡çš„æè¿°

| æœåŠ¡åç§°                         | è§’è‰²ä¸ä½œç”¨             | è¯´æ˜                                                         |
| -------------------------------- | ---------------------- | ------------------------------------------------------------ |
| **MONï¼ˆMonitorï¼‰**               | é›†ç¾¤ç®¡ç†å’ŒçŠ¶æ€ç»´æŠ¤     | è´Ÿè´£ç»´æŠ¤é›†ç¾¤çš„å¥åº·çŠ¶æ€ã€ç›‘æ§OSDã€ç®¡ç†é›†ç¾¤mapï¼ˆå¦‚OSDMapã€PGMapç­‰ï¼‰å’Œä»²è£åŠŸèƒ½ã€‚è‡³å°‘éƒ¨ç½²3ä¸ªä»¥å®ç°é«˜å¯ç”¨ã€‚ |
| **OSDï¼ˆObject Storage Daemonï¼‰** | å­˜å‚¨æ•°æ®çš„æ ¸å¿ƒç»„ä»¶     | æ¯ä¸ªOSDè´Ÿè´£å­˜å‚¨æ•°æ®ã€å¤„ç†æ•°æ®å¤åˆ¶ã€æ¢å¤ã€å›å¡«ã€å¿ƒè·³æ£€æµ‹ç­‰ï¼Œé›†ç¾¤ä¸­çš„ä¸»è¦å­˜å‚¨å•å…ƒã€‚ |
| **MGRï¼ˆManagerï¼‰**               | é›†ç¾¤ç›‘æ§ä¸ç®¡ç†æ‰©å±•åŠŸèƒ½ | æä¾›é›†ç¾¤ç›‘æ§ã€æ€§èƒ½è®¡é‡ã€Web UIæ¥å£ï¼ˆå¦‚Dashboardï¼‰ä»¥åŠç”¨äºæ’ä»¶çš„åŸºç¡€æ¶æ„ã€‚ |
| **MDSï¼ˆMetadata Serverï¼‰**       | ç®¡ç†CephFSå…ƒæ•°æ®       | ä¸“é—¨ä¸ºCephFSè®¾è®¡ï¼Œè´Ÿè´£æ–‡ä»¶ç³»ç»Ÿçš„å…ƒæ•°æ®ï¼ˆå¦‚ç›®å½•ç»“æ„ã€æƒé™ç­‰ï¼‰ï¼Œä¸å­˜å‚¨å®é™…æ–‡ä»¶æ•°æ®ã€‚ |
| **RGWï¼ˆRADOS Gatewayï¼‰**         | å¯¹è±¡ç½‘å…³æœåŠ¡           | æä¾›å…¼å®¹S3/Swiftçš„RESTfulæ¥å£ï¼Œå®ç°Cephå¯¹è±¡å­˜å‚¨çš„Webè®¿é—®ã€‚   |
| **NFS Ganesha**                  | NFSåè®®è®¿é—®Ceph        | æä¾›é€šè¿‡NFSv3/v4åè®®è®¿é—®CephFSæˆ–RBDçš„èƒ½åŠ›ï¼Œé€šè¿‡GaneshaæœåŠ¡å®ç°ã€‚ |

------

### Ceph æä¾›çš„å­˜å‚¨æ¥å£

| æ¥å£ç±»å‹                      | æè¿°                                   | ä½¿ç”¨åœºæ™¯                                     | ä¾èµ–æœåŠ¡                           |
| ----------------------------- | -------------------------------------- | -------------------------------------------- | ---------------------------------- |
| **CephFS**                    | åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿæ¥å£ï¼Œæ”¯æŒPOSIXè¯­ä¹‰      | é€‚åˆé«˜æ€§èƒ½è®¡ç®—ã€å…±äº«æ–‡ä»¶ç³»ç»Ÿã€ä¼ ç»Ÿæ–‡ä»¶è®¿é—®   | MDSã€MONã€OSD                      |
| **RBDï¼ˆRADOS Block Deviceï¼‰** | å—è®¾å¤‡æ¥å£ï¼Œæ”¯æŒå¿«ç…§ã€å…‹éš†ã€ç²¾ç®€é…ç½®ç­‰ | è™šæ‹Ÿæœºé•œåƒå­˜å‚¨ã€æ•°æ®åº“å­˜å‚¨ç­‰é«˜æ€§èƒ½å—è®¾å¤‡éœ€æ±‚ | MONã€OSD                           |
| **RGWï¼ˆRADOS Gatewayï¼‰**      | å¯¹è±¡å­˜å‚¨æ¥å£ï¼Œå…¼å®¹S3å’ŒSwift API        | äº‘å­˜å‚¨æœåŠ¡ã€å¤§æ•°æ®ã€å¤‡ä»½å½’æ¡£ç­‰å¯¹è±¡å­˜å‚¨åœºæ™¯   | MONã€OSDã€RGW                      |
| **NFSï¼ˆGanesha NFSï¼‰**        | æä¾›æ ‡å‡†NFSåè®®è®¿é—®Cephæ•°æ®            | ä¸ä¼ ç»ŸNFSå®¢æˆ·ç«¯å…¼å®¹çš„è®¿é—®æ–¹å¼                | Ganeshaã€CephFS æˆ– RGWï¼ˆé€šè¿‡å¯¼å‡ºï¼‰ |



## å‰ç½®æ¡ä»¶

- åŸºç¡€é…ç½®ï¼Œå®‰è£…æ–‡æ¡£å‚è€ƒï¼š[é“¾æ¥](/work/service/00-basic/)
- éœ€è¦ Python3ï¼Œå®‰è£…æ–‡æ¡£å‚è€ƒï¼š[é“¾æ¥](/work/service/python/v3.13.3/)ï¼Œç³»ç»Ÿå·²å­˜åœ¨å¯ä»¥å¿½ç•¥
- éœ€è¦ Dockerï¼Œå®‰è£…æ–‡æ¡£å‚è€ƒï¼š[é“¾æ¥](/work/docker/deploy/v27.3.1/)ï¼Œç³»ç»Ÿå·²å­˜åœ¨å¯ä»¥å¿½ç•¥

ç‰ˆæœ¬è¯´æ˜

| Ceph ç‰ˆæœ¬å· | å¯¹åº”ç‰ˆæœ¬åç§°           |
| ----------- | ---------------------- |
| 19.x        | Squid                  |
| 18.x        | Reef *ï¼ˆæ–‡æ¡£å½“å‰ç‰ˆæœ¬ï¼‰ |
| 17.x        | Quincy                 |
| 16.x        | Pacific                |

æœåŠ¡èŠ‚ç‚¹

| IP            | ä¸»æœºå                | è¯´æ˜       |
| ------------- | --------------------- | ---------- |
| 10.244.250.10 | service01 | ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ |
| 10.244.250.20 | service02 | æ–°å¢èŠ‚ç‚¹   |
| 10.244.250.30 | service03 | æ–°å¢èŠ‚ç‚¹   |



## åŸºç¡€é…ç½®

**å®‰è£…ä¾èµ–**

```
sudo yum install -y curl chrony lvm2
```

**æ—¶é—´åŒæ­¥**

é…ç½®æ—¶é—´åŒæ­¥ï¼Œè¯¦æƒ…å‚è€ƒæ—¶é—´åŒæ­¥æ–‡æ¡£ï¼š[é“¾æ¥](/work/service/chrony/)

```
sudo tee /etc/chrony.conf <<EOF
server ntp.aliyun.com iburst
server cn.ntp.org.cn iburst
driftfile /var/lib/chrony/drift
rtcsync
makestep 1.0 3
logdir /var/log/chrony
EOF
sudo systemctl enable --now chronyd
```

**å®‰è£…cephadm**

```
curl --silent --remote-name https://raw.githubusercontent.com/ceph/ceph/reef/src/cephadm/cephadm.py
chmod +x cephadm.py
sudo mv cephadm.py /usr/bin/cephadm
```

**æ‹‰å–é•œåƒ**

```
images="quay.io/ceph/ceph:v18
quay.io/ceph/ceph-grafana:9.4.7
quay.io/prometheus/prometheus:v2.43.0
quay.io/prometheus/alertmanager:v0.25.0
quay.io/prometheus/node-exporter:v1.5.0"
for image in $images
do
    docker pull $image
done
docker save $images | gzip -c > images-ceph_v18.tar.gz
```

**æ£€æŸ¥èŠ‚ç‚¹**

ç¡®è®¤å®¹å™¨å¼•æ“ï¼ˆå¦‚ podman æˆ– dockerï¼‰ã€lvmã€chronyd ç­‰æ˜¯å¦æ­£å¸¸ã€‚

```
cephadm check-host
```



## åˆå§‹åŒ–é›†ç¾¤

**åˆå§‹åŒ–ceph**

å¦‚æœæ“ä½œç³»ç»Ÿä¸æ”¯æŒï¼Œå¯ä»¥ä¿®æ”¹ `cephadm` æ–‡ä»¶å°†æ“ä½œç³»ç»ŸåŠ å…¥è¿›å»ï¼Œä»¥ OpenEuler ä¸ºä¾‹ï¼š`'openeuler': ('openeuler', 'el')`

RedHatç³»åˆ—ï¼švi +8057 /usr/bin/cephadm

Debianç³»åˆ—:   vi +7929 /usr/bin/cephadm

```
sudo cephadm bootstrap \
    --mon-ip 10.244.250.10 \
    --skip-mon-network \
    --cluster-network 10.244.250.0/24
```

- `--mon-ip 10.244.250.10`: æŒ‡å®šç”¨äºéƒ¨ç½²åˆå§‹ MONï¼ˆç›‘è§†å™¨ï¼‰å®ˆæŠ¤è¿›ç¨‹çš„ IP åœ°å€ï¼Œè¿™ä¸ª IP åº”è¯¥æ˜¯æœ¬æœºçš„ã€é›†ç¾¤èŠ‚ç‚¹ä¹‹é—´å¯è¾¾çš„åœ°å€ã€‚
- `--skip-mon-network`: ä¸è‡ªåŠ¨æ£€æµ‹å¹¶è®¾ç½® `public_network`ã€‚ä¼šå°† `mon-ip` æ‰€åœ¨çš„å­ç½‘ä½œä¸º `public_network` ä½¿ç”¨ã€‚é€‚ç”¨äºä½ ä¸æƒ³è®© Ceph è‡ªåŠ¨çŒœæµ‹ç½‘ç»œï¼Œæ‰‹åŠ¨æ§åˆ¶é…ç½®çš„æƒ…å†µã€‚
- `--cluster-network 10.244.250.0/24`: æŒ‡å®š Ceph é›†ç¾¤å†…éƒ¨ç”¨äº OSD ä¹‹é—´å¤åˆ¶æ•°æ®çš„ç½‘ç»œï¼ˆå³ `cluster_network`ï¼‰ã€‚è¿™æœ‰åŠ©äºæŠŠå®¢æˆ·ç«¯è®¿é—®æµé‡ï¼ˆ`public_network`ï¼‰å’Œé›†ç¾¤å†…éƒ¨æµé‡éš”ç¦»ï¼Œæé«˜æ€§èƒ½å’Œå®‰å…¨ã€‚

**é…ç½®public_network**

å®¢æˆ·ç«¯å’Œé›†ç¾¤ç»„ä»¶ä¹‹é—´çš„é€šä¿¡ï¼ˆå¦‚ MONã€MGRã€RGW ä¸å®¢æˆ·ç«¯ï¼‰

```
ceph config set mon public_network 10.244.250.0/24
ceph config get mon public_network
ceph orch restart mon
```

**è¿›å…¥ceph shell**

```
sudo cephadm shell
```

**æŸ¥çœ‹é›†ç¾¤çŠ¶æ€**

```
ceph status
```



## æ–°å¢èŠ‚ç‚¹

**æ‹·è´ç§˜é’¥**

```
ssh-copy-id -f -i /etc/ceph/ceph.pub root@service02
ssh-copy-id -f -i /etc/ceph/ceph.pub root@service03
```

**æ·»åŠ èŠ‚ç‚¹**

```
ceph orch host add service02 --labels _admin
ceph orch host add service03 --labels _admin
```

- `--labels _admin`ï¼šè®¾ç½®ä¸ºç®¡ç†èŠ‚ç‚¹

**æŸ¥çœ‹ä¸»æœºåˆ—è¡¨**

```
[root@service01 ~]# ceph orch host ls
HOST       ADDR           LABELS  STATUS  
service01  10.244.250.10  _admin          
service02  10.244.250.20  _admin          
service03  10.244.250.30  _admin          
3 hosts in cluster
```

**æŸ¥çœ‹ä¸»æœºçŠ¶æ€**

```
[root@service01 ~]# ceph cephadm check-host service02
service02 (None) ok
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit chronyd.service is enabled and running
Hostname "service02" matches what is expected.
Host looks OK
[root@service01 ~]# ceph cephadm check-host service03
service03 (None) ok
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit chronyd.service is enabled and running
Hostname "service03" matches what is expected.
Host looks OK
```



## åˆ›å»ºOSD

**æŸ¥çœ‹è®¾å¤‡**

```
ceph orch device ls
```

**åˆ›å»ºosd**

æ ¼å¼ï¼š`ceph orch daemon add osd <hostname>:<device>`

```
ceph orch daemon add osd service01:/dev/vdb
ceph orch daemon add osd service02:/dev/vdb
ceph orch daemon add osd service03:/dev/vdb
```

**æŸ¥çœ‹çŠ¶æ€**

```
[root@service01 ~]# ceph osd status
ID  HOST        USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      
 0  service01   290M  99.7G      0        0       0        0   exists,up  
 1  service02   290M  99.7G      0        0       0        0   exists,up  
 2  service03   690M  99.3G      0        0       0        0   exists,up  
```

**æŸ¥çœ‹poolä¿¡æ¯**

```
[root@service01 ~]# ceph df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
hdd    300 GiB  299 GiB  873 MiB   873 MiB       0.28
TOTAL  300 GiB  299 GiB  873 MiB   873 MiB       0.28
 
--- POOLS ---
POOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr   1    1  449 KiB        2  1.3 MiB      0     95 GiB
```

**æŸ¥çœ‹é›†ç¾¤çŠ¶æ€**

```
[root@service01 ~]# ceph status
  cluster:
    id:     551eb19a-2d8f-11f0-a391-8a34cfb79f78
    health: HEALTH_OK
 
  services:
    mon: 1 daemons, quorum service01 (age 16m)
    mgr: service01.puprig(active, since 14m), standbys: service02.qwmhwl
    osd: 3 osds: 3 up (since 35s), 3 in (since 54s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 449 KiB
    usage:   873 MiB used, 299 GiB / 300 GiB avail
    pgs:     1 active+clean
```



## è®¿é—® Dashboard

**æŸ¥çœ‹åœ°å€**

```
[ceph: root@ateng ~]# ceph mgr services
{
    "dashboard": "https://10.244.250.10:8443/",
    "prometheus": "http://10.244.250.10:9283/"
}
```

**åˆ›å»ºç›®å½•**

```
mkdir -p /data/ceph/password
```

**ä¿®æ”¹ç®¡ç†å‘˜å¯†ç **

```
echo "Admin@123" > /data/ceph/password/admin.password
ceph dashboard ac-user-set-password admin -i /data/ceph/password/admin.password
```

**æ–°å¢ç”¨æˆ·**

```
echo "Kongyu@123" > /data/ceph/password/kongyu.password
ceph dashboard ac-user-create kongyu -i /data/ceph/password/kongyu.password administrator
```



## ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿ

### åˆ›å»º CephFS

**åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ**

```
ceph fs volume create myfs
```

**æŸ¥çœ‹æ–‡ä»¶ç³»ç»Ÿ**

```
ceph fs ls
```

### ä½¿ç”¨ CephFS

**è·å–secret**

```
[root@service01 ~]# ceph auth get-key client.admin
AQCyJB9oN+bXGBAA0XKlug8bauPy4rD7vaUv6w==
```

**æŸ¥çœ‹monåœ°å€**

```
[root@service01 ~]# ceph auth get-key client.admin
AQCyJB9oN+bXGBAA0XKlug8bauPy4rD7vaUv6w==[root@service01 ~]# 
[root@service01 ~]# ceph mon dump
epoch 1
fsid 29e09146-2d86-11f0-a1c7-8a34cfb79f78
last_changed 2025-05-10T10:04:34.839846+0000
created 2025-05-10T10:04:34.839846+0000
min_mon_release 17 (quincy)
election_strategy: 1
0: [v2:10.244.250.10:3300/0,v1:10.244.250.10:6789/0] mon.service01
```

**å®‰è£…cephå®¢æˆ·ç«¯å·¥å…·**

```
sudo yum -y install ceph-common
```

**åˆ›å»ºç§˜é’¥**

```
mkdir -p /data/ceph/secret
echo "AQCyJB9oN+bXGBAA0XKlug8bauPy4rD7vaUv6w==" > /data/ceph/secret/admin.secret
chmod 600 /data/ceph/secret/admin.secret
```

**æŒ‚è½½cephæ–‡ä»¶ç³»ç»Ÿ**

,fsname=cephfs1

```
sudo mkdir -p /mnt/ceph-fs
sudo mount -t ceph service01:6789:/ /mnt/ceph-fs -o name=admin,secretfile=/data/ceph/secret/admin.secret
```

**æŸ¥çœ‹æŒ‚è½½**

```
[root@service02 ~]# df -hT /mnt/ceph-fs
Filesystem           Type  Size  Used Avail Use% Mounted on
10.244.250.10:6789:/ ceph   95G     0   95G   0% /mnt/ceph-fs
```

**å–æ¶ˆæŒ‚è½½**

```
sudo umount /mnt/ceph-fs
```



## ä½¿ç”¨å—è®¾å¤‡

### åˆ›å»º RBD

**åˆ›å»ºå—å­˜å‚¨æ± ï¼ˆpoolï¼‰**

128 æ˜¯ PGï¼ˆPlacement Groupsï¼‰æ•°é‡ï¼Œå¯æŒ‰é›†ç¾¤å¤§å°è°ƒæ•´ã€‚

```
ceph osd pool create rbdpool 128
```

**å¯ç”¨ RBD åŠŸèƒ½**

```
rbd pool init -p rbdpool
```

**åˆ›å»º RBD é•œåƒï¼ˆå—è®¾å¤‡ï¼‰**

åˆ›å»ºä¸€ä¸ª 10GB çš„å—è®¾å¤‡ï¼ˆ10240 MBï¼‰

```
rbd create myimage --size 10240 --pool rbdpool
```

**å®‰è£…å®¢æˆ·ç«¯å·¥å…·**

```
sudo yum install ceph-common -y
```

**åˆ›å»º Ceph é…ç½®æ–‡ä»¶å’Œè®¤è¯ keyring**

éœ€è¦æŠŠ Ceph ç®¡ç†èŠ‚ç‚¹ä¸Šçš„é…ç½®å’Œ keyring æ‹·è´åˆ°å®¢æˆ·ç«¯ï¼š

```
scp /etc/ceph/ceph.conf user@client:/etc/ceph/
scp /etc/ceph/ceph.client.admin.keyring user@client:/etc/ceph/
```

### ä½¿ç”¨ RBD

**æ˜ å°„ RBD åˆ°æœ¬åœ°è®¾å¤‡**

é€šå¸¸ä¼šæ˜¾ç¤º /dev/rbd0

```
rbd map myimage --pool rbdpool --name client.admin
```

**æ ¼å¼åŒ–å’ŒæŒ‚è½½å—è®¾å¤‡**

```
mkfs.ext4 /dev/rbd0
mkdir /mnt/rbdtest
mount /dev/rbd0 /mnt/rbdtest
```

**æŸ¥çœ‹æŒ‚è½½**

```
[root@service01 ~]# df -hT /mnt/rbdtest/
Filesystem     Type  Size  Used Avail Use% Mounted on
/dev/rbd0      ext4  9.8G   24K  9.3G   1% /mnt/rbdtest
```

**å¸è½½å’Œå–æ¶ˆæ˜ å°„**

```
umount /mnt/rbdtest
rbd unmap /dev/rbd0
```



## ä½¿ç”¨NFSå…±äº«æ–‡ä»¶

### åˆ›å»º NFS

**æŸ¥çœ‹é›†ç¾¤ä¸»æœº**

```
ceph orch host ls
```

**æŸ¥çœ‹ CephFS**

```
[root@service01 ~]# ceph fs ls 
name: myfs, metadata pool: cephfs.myfs.meta, data pools: [cephfs.myfs.data ]
```

**åˆ›å»º NFS Ganesha é›†ç¾¤**

è¿™é‡Œ `my-nfs` æ˜¯ Ganesha é›†ç¾¤åç§°ï¼Œ`service01` æ˜¯ä½ å¸Œæœ›è¿è¡Œ NFS çš„ä¸»æœºã€‚

```
ceph orch apply nfs my-nfs --placement="service01,service02,service03"
```

**æŸ¥çœ‹è¿è¡Œæƒ…å†µ**

```
[root@service01 ~]# ceph orch ps --daemon-type nfs
NAME                                   HOST            PORTS   STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
nfs.my-nfs.0.1.service01.xpkatq   service01  *:2049  running (8m)      8m ago   8m    41.0M        -  4.4      259b35566514  bb264f67b87e  
nfs.my-nfs.1.1.service03.opviwg   service03  *:2049  running (8m)      8m ago   8m    41.2M        -  4.4      259b35566514  9db27d2e11d1  
nfs.my-nfs.2.12.service02.zyuqgy  service02  *:2049  running (11s)     4s ago  11s    47.7M        -  4.4      259b35566514  cee8c56f337e  
```

**åˆ›å»ºå¯¼å‡ºç›®å½•**

```
ceph nfs export create cephfs my-nfs /nfs/myfs myfs /
```

- `my-nfs`ï¼šä½ åˆ›å»ºçš„ NFS Ganesha é›†ç¾¤åã€‚
- `/nfs/myfs`ï¼šNFS å®¢æˆ·ç«¯è®¿é—®çš„ä¼ªè·¯å¾„ï¼ˆpseudo pathï¼‰ã€‚
- `myfs`ï¼šCephFS æ–‡ä»¶ç³»ç»Ÿçš„åå­—ã€‚
- `/`ï¼šå®é™… CephFS ä¸­å¯¼å‡ºçš„è·¯å¾„ï¼ˆå¦‚æ ¹ç›®å½•ï¼‰ã€‚

### æŸ¥çœ‹ä¿¡æ¯

**æŸ¥çœ‹å¯¼å‡ºé…ç½®**

```
[root@service01 ~]# ceph nfs export ls my-nfs
[
  "/nfs/myfs"
]
```

**æŸ¥çœ‹å¯¼å‡ºåœ°å€**

```
[root@service01 ~]# ceph nfs cluster info my-nfs
{
    "my-nfs": {
        "virtual_ip": null,
        "backend": [
            {
                "hostname": "service01",
                "ip": "10.244.250.10",
                "port": 2049
            },
            {
                "hostname": "service03",
                "ip": "10.244.250.30",
                "port": 2049
            }
        ]
    }
}
```

### ä½¿ç”¨ NFS

**å®‰è£…NFSå®¢æˆ·ç«¯å·¥å…·**

```
sudo yum -y install nfs-utils
```

**æŒ‚è½½NFS**

```
mkdir /mnt/ceph-nfs
mount -t nfs -o vers=4 service01:/nfs/myfs /mnt/ceph-nfs
```

**æŸ¥çœ‹æŒ‚è½½**

```
[root@service01 ~]# df -hT /mnt/ceph-nfs/
Filesystem               Type  Size  Used Avail Use% Mounted on
service01:/nfs/myfs nfs4   95G     0   95G   0% /mnt/ceph-nfs
```

**å–æ¶ˆæŒ‚è½½**

```
umount /mnt/ceph-nfs/
```



## ä½¿ç”¨å¯¹è±¡å­˜å‚¨

### åˆ›å»ºRGW

**åˆ›å»ºRGW**

æ ¼å¼ï¼š`ceph orch apply rgw <instance-name> --placement="<ä¸»æœºå>[,...]"`

```
ceph orch apply rgw default --placement="service01,service02,service03"
```

**æŸ¥çœ‹è¿è¡Œæƒ…å†µ**

```
[root@service01 ~]# ceph orch ps --daemon-type rgw
NAME                               HOST            PORTS  STATUS        REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
rgw.default.service01.rizmsp  service01  *:80   running (2m)     2m ago   2m    10.8M        -  17.2.8   259b35566514  e4babfed73f8  
rgw.default.service02.cnduxx  service02  *:80   running (2m)     2m ago   2m    9432k        -  17.2.8   259b35566514  7b37f84ebf99  
rgw.default.service03.pjohfh  service03  *:80   running (2m)     2m ago   2m    10.8M        -  17.2.8   259b35566514  f4adb624ea0e  
```

**åˆ›å»ºè®¿é—®ç”¨æˆ·**

```
radosgw-admin user create --uid=testuser --display-name="Test User"
```

**æŸ¥çœ‹ç”¨æˆ·ä¿¡æ¯**

```
[root@service01 ~]# radosgw-admin user info --uid=testuser
{
    "user_id": "testuser",
    "display_name": "Test User",
    "email": "",
    "suspended": 0,
    "max_buckets": 1000,
    "subusers": [],
    "keys": [
        {
            "user": "testuser",
            "access_key": "3P5G5RMU30KU1B35JJDK",
            "secret_key": "v5wjWiCyshLxJEFTlaqfosLlialChmhLVNGDcl7D"
        }
    ],
    "swift_keys": [],
    "caps": [],
    "op_mask": "read, write, delete",
    "default_placement": "",
    "default_storage_class": "",
    "placement_tags": [],
    "bucket_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "user_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "temp_url_keys": [],
    "type": "rgw",
    "mfa_ids": []
}
```

### ä½¿ç”¨RGW

**ä½¿ç”¨å®¢æˆ·ç«¯å·¥å…·è¿æ¥**

```
mcli config host add ceph-rgw http://10.244.250.10 3P5G5RMU30KU1B35JJDK v5wjWiCyshLxJEFTlaqfosLlialChmhLVNGDcl7D --api s3v4
```

**ä¸Šä¼ æ–‡ä»¶**

```
mcli mb ceph-rgw/ateng
mcli cp README.md ceph-rgw/ateng
```

**æŸ¥çœ‹æ–‡ä»¶**

```
[root@server02 quincy]# mcli ls ceph-rgw/ateng
[2025-05-10 16:26:28 CST]  14KiB STANDARD README.md
```



## åˆ›å»ºé›†ç¾¤

### æŸ¥çœ‹é›†ç¾¤ä¸»æœº

```
[root@service01 ~]# ceph orch host ls
HOST                   ADDR           LABELS  STATUS  
service01  10.244.250.10  _admin          
service02  10.244.250.20  _admin          
service03  10.244.250.30  _admin          
3 hosts in cluster
```

### æ·»åŠ MON

**æ·»åŠ MON**

```
ceph orch daemon add mon service02
ceph orch daemon add mon service02
```

**æŸ¥çœ‹è¿è¡Œæƒ…å†µ**

```
[root@service01 ~]# ceph orch ps --daemon-type mon
NAME           HOST       PORTS  STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
mon.service01  service01         running (14m)     5m ago  49m    43.1M    2048M  17.2.8   259b35566514  4265ac78b7ea  
mon.service02  service02         running (79s)    73s ago  79s    27.6M    2048M  17.2.8   259b35566514  e8efd35339ae  
mon.service03  service03         running (71s)    46s ago  71s    32.7M    2048M  17.2.8   259b35566514  67a0cae818e7 
```

### æ·»åŠ MGR

**åˆ›å»ºMGR**

```
ceph orch apply mgr --placement="service01,service02,service03"
```

**æŸ¥çœ‹è¿è¡Œæƒ…å†µ**

```
[root@service01 ~]# ceph orch ps --daemon-type mgr
NAME                  HOST                   PORTS             STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
mgr.service01.dohmno  service01  *:9283,8765,8443  running (54m)    97s ago  54m     442M        -  17.2.8   259b35566514  e910f664caa4  
mgr.service02.olslaz  service02  *:8443,9283       running (44m)    30s ago  44m     391M        -  17.2.8   259b35566514  57b469e272c5  
mgr.service03.askrim  service03  *:8443,9283       running (9s)      3s ago   9s     172M        -  17.2.8   259b35566514  049eceb786eb  
```

### æ·»åŠ MDS

**æŸ¥çœ‹ CephFS**

```
[root@service01 ~]# ceph fs ls 
name: myfs, metadata pool: cephfs.myfs.meta, data pools: [cephfs.myfs.data ]
```

**æŸ¥çœ‹è¿è¡Œæƒ…å†µ**

åˆ›å»ºäº†fsé»˜è®¤å°±æœ‰ä¸€ä¸ª MDS å®ˆæŠ¤è¿›ç¨‹

```
[root@service01 ~]# ceph orch ps --daemon-type mds
NAME                            HOST            PORTS  STATUS        REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
mds.myfs.service01.beshsk  service01         running (2m)     2m ago   2m    13.9M        -  17.2.8   259b35566514  2e563f98831f  
```

**æ·»åŠ mds**

ä¸€ä¸ª Ceph æ–‡ä»¶ç³»ç»Ÿï¼ˆFSï¼‰æœ€å°‘éœ€è¦éƒ¨ç½²ä¸€ä¸ª MDS å®ˆæŠ¤è¿›ç¨‹

æ ¼å¼ï¼š`ceph orch apply mds <fs-name> --placement="<ä¸»æœºå>[,...]"`

```
ceph orch apply mds myfs --placement="service01,service02,service03"
```

**æŸ¥çœ‹è¿è¡Œæƒ…å†µ**

åˆ›å»ºäº†fsé»˜è®¤å°±æœ‰ä¸€ä¸ª MDS å®ˆæŠ¤è¿›ç¨‹

```
[root@service01 ~]# ceph orch ps --daemon-type mds
NAME                       HOST                   PORTS  STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
mds.myfs.service01.keymak  service01         running (52m)    11s ago  52m    24.4M        -  17.2.8   259b35566514  f3adc823901c  
mds.myfs.service02.qfridx  service02         running (46m)   106s ago  46m    22.9M        -  17.2.8   259b35566514  cb4bde69ee59  
mds.myfs.service03.jhclqc  service03         running (8s)      2s ago   9s    12.3M        -  17.2.8   259b35566514  72c684ab6a02  
```

**æŸ¥çœ‹çŠ¶æ€**

```
[root@service01 ~]# ceph mds stat
myfs:1 {0=myfs.service01.beshsk=up:active} 2 up:standby
```



## æŸ¥çœ‹é›†ç¾¤

åœ¨ä½¿ç”¨ Ceph åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿæ—¶ï¼Œå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå·¥å…· `ceph` æ¥æŸ¥çœ‹é›†ç¾¤çš„å„ç§çŠ¶æ€å’Œä¿¡æ¯ã€‚ä¸‹é¢æ˜¯ä¸€äº›å¸¸ç”¨çš„ Ceph é›†ç¾¤æŸ¥çœ‹å‘½ä»¤ï¼Œé€‚ç”¨äºç®¡ç†å‘˜æ—¥å¸¸æ’æŸ¥ä¸ç»´æŠ¤ã€‚

------

ğŸ“‹ åŸºæœ¬é›†ç¾¤çŠ¶æ€

| å‘½ä»¤                       | è¯´æ˜                                            |
| -------------------------- | ----------------------------------------------- |
| `ceph -s` æˆ– `ceph status` | æŸ¥çœ‹é›†ç¾¤çš„æ•´ä½“çŠ¶æ€ï¼ˆå¥åº·çŠ¶æ€ã€OSDã€MONã€PG ç­‰ï¼‰ |
| `ceph health`              | æ˜¾ç¤ºé›†ç¾¤å¥åº·çŠ¶æ€ï¼ˆHEALTH_OK / WARN / ERRï¼‰      |
| `ceph df`                  | æŸ¥çœ‹é›†ç¾¤å­˜å‚¨ä½¿ç”¨æƒ…å†µï¼ˆæ€»å®¹é‡ã€å·²ç”¨ã€å¯ç”¨ï¼‰      |
| `ceph osd df`              | æŸ¥çœ‹å„ä¸ª OSD çš„å®¹é‡ä½¿ç”¨æƒ…å†µ                     |
| `ceph osd pool stats`      | æŸ¥çœ‹å„ä¸ªæ± çš„è¯»å†™é€Ÿç‡å’Œå»¶è¿Ÿä¿¡æ¯                  |

------

ğŸ§  ç›‘æ§ç»„ä»¶ï¼ˆMON/MGRï¼‰

| å‘½ä»¤            | è¯´æ˜                               |
| --------------- | ---------------------------------- |
| `ceph mon stat` | æŸ¥çœ‹ MON èŠ‚ç‚¹çš„æ•°é‡å’ŒçŠ¶æ€          |
| `ceph mgr stat` | æŸ¥çœ‹å½“å‰æ´»åŠ¨çš„ MGRï¼ˆç®¡ç†å®ˆæŠ¤è¿›ç¨‹ï¼‰ |

------

ğŸ’¾ OSD ç›¸å…³å‘½ä»¤

| å‘½ä»¤                   | è¯´æ˜                                 |
| ---------------------- | ------------------------------------ |
| `ceph osd stat`        | æŸ¥çœ‹ OSD æ€»æ•°å’Œ up/down çŠ¶æ€         |
| `ceph osd tree`        | æŸ¥çœ‹ OSD çš„æ‹“æ‰‘ç»“æ„ï¼ˆhostã€rack ç­‰ï¼‰ |
| `ceph osd crush tree`  | æŸ¥çœ‹ CRUSH è§„åˆ™ä¸‹çš„ OSD ç»“æ„         |
| `ceph osd perf`        | æŸ¥çœ‹ OSD çš„å»¶è¿Ÿæ€§èƒ½æ•°æ®              |
| `ceph osd utilization` | æŸ¥çœ‹æ¯ä¸ª OSD çš„åˆ©ç”¨ç‡                |

------

ğŸ“¦ PGï¼ˆPlacement Groupï¼‰ç›¸å…³

| å‘½ä»¤                     | è¯´æ˜                            |
| ------------------------ | ------------------------------- |
| `ceph pg stat`           | æ˜¾ç¤º PG çŠ¶æ€ï¼ˆactive+clean ç­‰ï¼‰ |
| `ceph pg dump`           | å¯¼å‡ºæ‰€æœ‰ PG çš„è¯¦ç»†ä¿¡æ¯          |
| `ceph pg dump pgs_brief` | ç®€è¦å±•ç¤º PG çŠ¶æ€ï¼ˆæ¨èï¼‰        |

------

ğŸŠ Poolï¼ˆå­˜å‚¨æ± ï¼‰ç›¸å…³

| å‘½ä»¤                                | è¯´æ˜                 |
| ----------------------------------- | -------------------- |
| `ceph osd lspools`                  | åˆ—å‡ºæ‰€æœ‰å­˜å‚¨æ±        |
| `ceph osd pool ls detail`           | æŸ¥çœ‹æ‰€æœ‰æ± çš„è¯¦ç»†ä¿¡æ¯ |
| `ceph osd pool get <pool-name> all` | æŸ¥çœ‹æŒ‡å®šæ± çš„å…¨éƒ¨å‚æ•° |

------

ğŸ§ª å…¶ä»–å¸¸ç”¨å‘½ä»¤

| å‘½ä»¤                                      | è¯´æ˜                              |
| ----------------------------------------- | --------------------------------- |
| `ceph fs status`                          | æŸ¥çœ‹ CephFS æ–‡ä»¶ç³»ç»ŸçŠ¶æ€          |
| `ceph mds stat`                           | æŸ¥çœ‹å…ƒæ•°æ®æœåŠ¡å™¨ï¼ˆMDSï¼‰çŠ¶æ€       |
| `ceph versions`                           | æŸ¥çœ‹é›†ç¾¤ä¸­å„ç»„ä»¶çš„ç‰ˆæœ¬            |
| `ceph quorum_status --format json-pretty` | æŸ¥çœ‹ MON çš„ä¸€è‡´æ€§çŠ¶æ€ï¼ˆé€‚åˆè°ƒè¯•ï¼‰ |



## åˆ é™¤é›†ç¾¤

**è·å–é›†ç¾¤çš„ FSID**

```
[root@service01 ~]# ceph fsid
80d01902-2d47-11f0-863e-8a34cfb79f78
```

**å¸è½½é›†ç¾¤**

æ¯ä¸ªèŠ‚ç‚¹éƒ½è¦æ‰§è¡Œ

```
[root@service01 ~]# cephadm rm-cluster --fsid 29e09146-2d86-11f0-a1c7-8a34cfb79f78 --force --zap-osds
```

- `--force`ï¼šå¼ºåˆ¶åˆ é™¤é›†ç¾¤ï¼Œé€šå¸¸ç”¨äºé›†ç¾¤çŠ¶æ€ä¸ç¨³å®šæ—¶ã€‚
- `--keep-logs`ï¼šä¿ç•™æ—¥å¿—æ–‡ä»¶ã€‚
- `--zap-osds`ï¼šåˆ é™¤æ‰€æœ‰ OSD æ•°æ®ã€‚
